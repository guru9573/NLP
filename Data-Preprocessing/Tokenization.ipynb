{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tokenization.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OsZ4khO4qnP"
      },
      "source": [
        "# Tokenization\n",
        "\n",
        "Tokenization is the process of breaking a stream of textual data into words, terms, sentences, symbols, or some other meaningful elements called tokens. These tokens can be represented as Vectors which can be fed into Machine Learning models.\n",
        "\n",
        "## Why Tokenization?\n",
        "\n",
        "In order to get our computer to understand any text, we need to break the sentences down in a way that our machine can understand. Thatâ€™s where the concept of tokenization in Natural Language Processing (NLP) comes in. So, this is the first step to build NLP model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-Z0Lv343myh"
      },
      "source": [
        "## Installation of nltk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yahQdhZs3jTd",
        "outputId": "68052db9-ee0d-4e35-a124-20a7072f30dd"
      },
      "source": [
        "!pip install nltk"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3onSh3_3v6Q"
      },
      "source": [
        "# nltk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wq6eAaRp3U9i"
      },
      "source": [
        "import nltk\n",
        "\n",
        "from nltk import word_tokenize, sent_tokenize"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Z_wGVfr7MQ9",
        "outputId": "cafe6533-f4ca-42d7-aad7-89570d3a098c"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6Zl90DG3b2E"
      },
      "source": [
        "text = \"Data Science. Machine Learning. Deep Learning.\""
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jVObdos_vBh"
      },
      "source": [
        "## Word Tokenization\n",
        "* Word tokenization is the most used Tokenization algorithm. It splits text into words based on certain delimiter (generally \"space\")\n",
        "* This can be accomplished using word_tokenize method from nltk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u44LFWDY7HBH"
      },
      "source": [
        "word_tokens = word_tokenize(text)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewpvp1Dg7qfQ",
        "outputId": "9569cebb-5b72-49ff-b30e-306c7273c6dd"
      },
      "source": [
        "print(word_tokens)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Data', 'Science', '.', 'Machine', 'Learning', '.', 'Deep', 'Learning', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMloBZ_vCbVq"
      },
      "source": [
        "## Sentence Tokenization\n",
        "* Sentence Tokenization breaks a paragraph into sentences\n",
        "* An obvious question youu might get is, why sentence tokenization is required if we have Word Tokenization. Suppose, the task is to calculate average number of words in sentence, we can make use of Sentence Tokenization.\n",
        "* This can be accomplished using sent_tokenize from nltk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GExmOHG96236"
      },
      "source": [
        "sent_tokens = sent_tokenize(text)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBciEvCz7yo6",
        "outputId": "2dbae4a9-8871-4bb0-9cf1-629ca3ae08e9"
      },
      "source": [
        "print(sent_tokens)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Data Science.', 'Machine Learning.', 'Deep Learning.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DZWH68q7727"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_x6m8V2zDvF0"
      },
      "source": [
        "# spacy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUti8JuLDwf6",
        "outputId": "6777e861-bb6a-4754-d938-693a8f147168"
      },
      "source": [
        "!pip install spacy"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.6.4)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.5.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bF9CSYyEDzCq"
      },
      "source": [
        "from spacy.lang.en import English"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLb61mmREHBc"
      },
      "source": [
        "nlp = English()"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bfae6y7TEWHt"
      },
      "source": [
        "## Word Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCrFE7ePELVu"
      },
      "source": [
        "doc = nlp(text)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dobiVzfgEYII"
      },
      "source": [
        "word_tokens = []\n",
        "\n",
        "for token in doc:\n",
        "  word_tokens.append(token.text)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sh1-tRxmE7pH",
        "outputId": "0be7dda2-83c6-4ca4-fe66-eebe76a766b7"
      },
      "source": [
        "print(word_tokens)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Data', 'Science', '.', 'Machine', 'Learning', '.', 'Deep', 'Learning', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDwDiaEKFe_y"
      },
      "source": [
        "## Sentence Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kae6bLYPFhmr"
      },
      "source": [
        "pipe = nlp.create_pipe('sentencizer')\n",
        "\n",
        "nlp.add_pipe(pipe)\n",
        "\n",
        "doc = nlp(text)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EbR5dFKE9m8"
      },
      "source": [
        "sent_tokens = []\n",
        "\n",
        "for token in doc.sents:\n",
        "  sent_tokens.append(token)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08JHCEmDFTrM",
        "outputId": "0c40d25a-a994-48dd-f2a0-7f559c01c4ba"
      },
      "source": [
        "print(sent_tokens)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Data Science., Machine Learning., Deep Learning.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpdfVhgAF6Q8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}